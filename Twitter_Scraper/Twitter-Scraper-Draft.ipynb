{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locationtagger\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "import os\n",
    "# from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import certifi; \n",
    "\n",
    "\n",
    "# essential entity models downloads\n",
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "nltk.downloader.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-12-22'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datetime.today() - timedelta(days=30)).strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTweetData(tweet):\n",
    "    try:\n",
    "        tweetTime = tweet.find_element(\n",
    "            By.XPATH, \".//time\").get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    try:\n",
    "        # This only gets one image, what if there are more?\n",
    "        tweetImage = tweet.find_element(\n",
    "            By.XPATH, \".//div[1]/div[1]//div[2]/div[2]//img\").get_attribute(\"src\")\n",
    "    except NoSuchElementException:\n",
    "        # print(\"no image\")\n",
    "        tweetImage = \"No Image\"\n",
    "    tweetText = tweet.find_element(\n",
    "        By.XPATH, \".//div[1]/div[1]/div[2]/div[2]/div[2]\").text\n",
    "\n",
    "    tweetInfo = [tweetText, tweetTime, tweetImage]\n",
    "    return tweetInfo\n",
    "\n",
    "\n",
    "def ScraperMain():\n",
    "    twitterUsername = os.getenv('TW_USERNAME')\n",
    "    twitterPassword = os.getenv('TW_PASSWORD')\n",
    "    \n",
    "\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    # driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get('https://www.twitter.com/login')\n",
    "    sleep(3)\n",
    "    # Finding and inputing username\n",
    "    username = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[1]/div[5]/label[1]/div[1]/div[2]/div[1]/input[1]\")\n",
    "    username.send_keys(twitterUsername)\n",
    "    username.send_keys(Keys.RETURN)\n",
    "    sleep(1)\n",
    "    # Finding and inputing Password\n",
    "    # mypassword = getpass()\n",
    "    password = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[2]/div[2]/div[1]/div[1]/div[1]/div[3]/div[1]/label[1]/div[1]/div[2]/div[1]/input[1]\")\n",
    "    password.send_keys(twitterPassword)\n",
    "    password.send_keys(Keys.RETURN)\n",
    "    sleep(5)\n",
    "\n",
    "    # Selecting and searching required police section\n",
    "    search = driver.find_element(\n",
    "        By.XPATH, \"//body/div[@id='react-root']/div[1]/div[1]/div[2]/main[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/form[1]/div[1]/div[1]/div[1]/div[1]/label[1]/div[2]/div[1]/input[1]\")\n",
    "    search.send_keys('@TPSOperations')\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    sleep(2)\n",
    "\n",
    "    # Click required page\n",
    "    SearchQuery = driver.find_element(\n",
    "        By.XPATH, \"//span[contains(text(),'Toronto Police Operations')]\")\n",
    "    SearchQuery.click()\n",
    "\n",
    "    # Storing Tweet data in list\n",
    "    tweetData = []\n",
    "    tweetIds = set()\n",
    "    lastPos = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    scrolling = True\n",
    "    maxDate = False\n",
    "    stopDate = str((datetime.today() - timedelta(days=3)).strftime('%Y-%m-%d'))\n",
    "\n",
    "    while (scrolling and not (maxDate)):\n",
    "        # Find Tweets\n",
    "        TPOPageTweets = driver.find_elements(\n",
    "            By.XPATH, '//article[@data-testid=\"tweet\"]')\n",
    "        sleep(1)\n",
    "        for tweet in TPOPageTweets[-100:]:\n",
    "            currentTweetInfo = getTweetData(tweet)\n",
    "            if currentTweetInfo:\n",
    "                tweetId = \"\".join(currentTweetInfo)\n",
    "                if tweet not in tweetIds and currentTweetInfo[0] != '' and \":\" in currentTweetInfo[0] and 'Good night' not in currentTweetInfo[0] and 'Good afternoon' not in currentTweetInfo[0] and 'Good morning' not in currentTweetInfo[0] and 'Good Night' not in currentTweetInfo[0]:\n",
    "                    tweetIds.add(tweet)\n",
    "                    currentTweetInfo.append(tweetId)\n",
    "                    tweetData.append(currentTweetInfo)\n",
    "                    if stopDate in currentTweetInfo[1]:\n",
    "                        maxDate = True\n",
    "                        break\n",
    "\n",
    "        scrollAttempt = 0\n",
    "        while True:\n",
    "            driver.execute_script('window.scrollBy(0,3000);')\n",
    "            sleep(2)\n",
    "            currPos = driver.execute_script(\"return window.pageYOffset;\")\n",
    "            if maxDate:\n",
    "                break\n",
    "            if lastPos == currPos:\n",
    "                scrollAttempt += 1\n",
    "                if scrollAttempt >= 3:\n",
    "                    scrolling = False\n",
    "                    break\n",
    "                else:\n",
    "                    sleep(2)\n",
    "            else:\n",
    "                lastPos = currPos\n",
    "                break\n",
    "    print(\"Tweets Scraped\")\n",
    "    return tweetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Scraped\n"
     ]
    }
   ],
   "source": [
    "tweets = ScraperMain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortData(scrapedTweet):\n",
    "    # List: catagory (Missing...), Status(Update, Located)\n",
    "    computedData = []\n",
    "    counter = 0\n",
    "    for i in range(len(scrapedTweet)):\n",
    "        dataObject = {}\n",
    "        if (scrapedTweet[i][0].find('\\n') != -1):\n",
    "            # Getting Status:\n",
    "            dataObject[\"Status\"] = scrapedTweet[i][0].splitlines()[0].split(\":\")[0]\n",
    "            # Getting Updates (?LOCATED)...\n",
    "            try:\n",
    "                dataObject[\"Updates\"] = scrapedTweet[i][0].splitlines()[0].split(\":\")[1]\n",
    "            except IndexError:\n",
    "                dataObject[\"Updates\"] = \"\"\n",
    "            # currentLocation could be the currentIdentity\n",
    "            currentLocation = scrapedTweet[i][0].splitlines()[1].split(\", \")\n",
    "            # Checking whether we have a Location or an identity\n",
    "            if any(char.isdigit() for char in currentLocation):\n",
    "                dataObject[\"Name\"] = currentLocation[0]\n",
    "                if currentLocation[1].isnumeric():\n",
    "                    dataObject[\"Age\"] = currentLocation[1]\n",
    "                else:\n",
    "                    dataObject[\"Age\"] = \"\"\n",
    "                description = scrapedTweet[i][0].splitlines()\n",
    "                del description[0:2]\n",
    "                #Extracting Location from Description\n",
    "                lists = scrapedTweet[i][0].splitlines()\n",
    "                del lists[0:2]\n",
    "                try:\n",
    "                    partOfDescription = lists[0] + \" \" + lists[1]\n",
    "                except IndexError:\n",
    "                    partOfDescription = scrapedTweet[i][0]                   \n",
    "                dataObject[\"Location\"] = locationtagger.find_locations(text = partOfDescription).other\n",
    "            else:\n",
    "                try:\n",
    "                    dataObject[\"Location\"] = scrapedTweet[i][0].splitlines()[1]\n",
    "                except IndexError:\n",
    "                    dataObject[\"Location\"] = \"\"\n",
    "                description = scrapedTweet[i][0].splitlines()\n",
    "                del description[0:1]\n",
    "            if dataObject[\"Location\"] is not None:\n",
    "                locationcheck = dataObject[\"Location\"]\n",
    "                if type(dataObject[\"Location\"]) == list and bool(dataObject[\"Location\"]):\n",
    "                    res = max(locationcheck, key = len)\n",
    "                    locationcheck = res\n",
    "                if \"and\" in locationcheck:\n",
    "                    locationcheck = locationcheck.replace(\"and\", \"\")\n",
    "                if \"&\" in locationcheck:\n",
    "                    locationcheck = locationcheck.replace(\"&\", \"\")\n",
    "                # geolocator = Nominatim(user_agent=\"Twitter_Scraper\")\n",
    "                # geocode = lambda query: geolocator.geocode(\"%s, Toronto ON\" % query)\n",
    "                # geocode2 = RateLimiter(geocode, min_delay_seconds=0.001)\n",
    "                sleep(1)\n",
    "                # locationcor = geocode2(locationcheck)\n",
    "                locationcor = None\n",
    "                if locationcor is not None:\n",
    "                    locationcor2 = [locationcor.latitude, locationcor.longitude]\n",
    "                    dataObject[\"LocationGoeCode\"] = locationcor2\n",
    "                else:\n",
    "                    dataObject[\"LocationGoeCode\"] = []\n",
    "                    \n",
    "            else:\n",
    "                dataObject[\"LocationGoeCode\"] = [\"No Location Provided\"]\n",
    "            \n",
    "            input_datetime = datetime.strptime(scrapedTweet[i][1], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            # Format as MM-DD-YYYY\n",
    "            output_date_string = input_datetime.strftime(\"%m-%d-%Y\")\n",
    "            dataObject[\"TweetedTime\"] = output_date_string\n",
    "            dataObject[\"ImageUrl\"] = scrapedTweet[i][2]\n",
    "            dataObject[\"Description\"] = description\n",
    "            computedData.append(dataObject)\n",
    "            counter = counter + 1\n",
    "            # print(counter)\n",
    "            # print(dataObject['Location'])\n",
    "            # print(dataObject['LocationGoeCode'])\n",
    "            print(description)\n",
    "        else:\n",
    "            print(\"Tweet Skipped: \", scrapedTweet[i])\n",
    "    print(\"Tweets Sorted\")\n",
    "    return(computedData)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- please remove all photos from circulation', '#GO170627', '^se']\n",
      "['Please remove all images of them from circulation', '#GO159196', '^lb']\n",
      "['Milner/Morningside', '11:40 a.m.', '- Reports that what appear to be bullet holes found in the window of a theater', '- Unknown when this occurred', '- Police are on scene', '- Evidence of gunfire located', '- No victims', '- Anyone w/info contact police ', '@TPS42Div', 'Show more']\n",
      "['Wilson Av + Allen Rd area', '11:28 a.m.', '- Police have responded to reports of gunshots heard in the area', '- St. Norbert Catholic Elementary School is in lockdown', '- Police are on scene ', '@TPS32Div', '- Anyone w/info contact police', '#GO177655', '^lb']\n",
      "['Wilson Av + Allen Rd area', '- Police checked the area and located firecrackers ', '- School is out of lockdown', '^lb']\n",
      "['Please remove all images of them from circulation', '#GO162725', '^lb']\n",
      "['Please remove all images of them from circulation', '#GO157956', '^lb']\n",
      "['Scarsdale Rd + York Mills Rd', '9:09 a.m.', '- Reports of icy road conditions', '- 4 vehicles involved', '- No reports of injuries', '- Use caution in the area', '- Salter requested to location', '#GO176601', '^lb']\n",
      "['Please remove all images of them from circulation', '#GO121552', '^lb']\n",
      "['Wynford Dr & Eglinton Ave E', '11:23pm', '- reports of a number of trees on fire', '- ', '@Toronto_Fire', ' & police are o/s', '- no reported injuries', '- delays in the area', '- consider alternate routes', '#GO174530']\n",
      "['Hwy 27 S & Dixon Rd E Ramp', '9:43pm', '- reports of a two vehicle collision', '- one vehicle rolled over into ditch', '- other vehicle fled the scene', '- no reported injuries', '- police are o/s', '- possible delays in the area', '- consider alternate routes', 'Show more']\n",
      "['Hwy 427 N & Disco Rd', '8:24pm', '- Mississauga Transit bus into a hydro pole', '- driver was alone, no passengers on bus', '- polife, fire & medics are o/s', '- Hydro has been notified', '- driver has been transported to hospital with minor injuries', '#GO173671', '^se']\n",
      "['Hwy 27 S & Dixon Rd E Ramp', '9:43pm', '- reports of a two vehicle collision', '- one vehicle rolled over into ditch', '- other vehicle fled the scene', '- no reported injuries', '- police are o/s', '- possible delays in the area', '- consider alternate routes', 'Show more']\n",
      "['Hwy 427 N & Disco Rd', '8:24pm', '- Mississauga Transit bus into a hydro pole', '- driver was alone, no passengers on bus', '- polife, fire & medics are o/s', '- Hydro has been notified', '- driver has been transported to hospital with minor injuries', '#GO173671', '^se']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msortData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 53\u001b[0m, in \u001b[0;36msortData\u001b[1;34m(scrapedTweet)\u001b[0m\n\u001b[0;32m     49\u001b[0m     locationcheck \u001b[38;5;241m=\u001b[39m locationcheck\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# geolocator = Nominatim(user_agent=\"Twitter_Scraper\")\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# geocode = lambda query: geolocator.geocode(\"%s, Toronto ON\" % query)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# geocode2 = RateLimiter(geocode, min_delay_seconds=0.001)\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# locationcor = geocode2(locationcheck)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m locationcor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sortData(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addToDB(computedData):\n",
    "    MONGODB_PASS = os.getenv('MONGODB_PASS')\n",
    "\n",
    "    CONNECTION_STRING = \"mongodb+srv://OCMB1:\" + MONGODB_PASS + \\\n",
    "        \"@cluster0.e5wloiz.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "    client = MongoClient(CONNECTION_STRING, tlsCAFile=certifi.where())\n",
    "\n",
    "    if 'Toronto_Police_Crime_Report' in client.list_database_names():\n",
    "        client.drop_database('Toronto_Police_Crime_Report')\n",
    "\n",
    "    db = client['Toronto_Police_Crime_Report']\n",
    "    collection_name = db[\"tweetsData\"]\n",
    "    res = collection_name.insert_many(computedData)\n",
    "    print(\"Added To DataBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Scraped\n"
     ]
    }
   ],
   "source": [
    "tweets = ScraperMain()\n",
    "# sortedData = sortData(tweets)\n",
    "# addToDB(sortedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedData = sortData(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added To DataBase\n"
     ]
    }
   ],
   "source": [
    "addToDB(sortedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"Twitter\")\n",
    "geocode = lambda query: geolocator.geocode(\"%s, Toronto ON\" % query)\n",
    "location = geocode(\"Victoria Park Van Horne\")\n",
    "location2 = [location.latitude, location.longitude]\n",
    "location2\n",
    "# print((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace YOUR_API_KEY with your actual API key. Sign up and get an API key on https://www.geoapify.com/ \n",
    "API_KEY = \"e904244da7514e13a457430791adcf16\"\n",
    "\n",
    "# Define the address to geocode\n",
    "address = \"Jane St Wilson Ave area, Toronto, CA\"\n",
    "\n",
    "# Build the API URL\n",
    "url = f\"https://api.geoapify.com/v1/geocode/search?text={address}&limit=1&apiKey={API_KEY}\"\n",
    "\n",
    "# Send the API request and get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON data from the response\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the first result from the data\n",
    "    result = data[\"features\"][0]\n",
    "\n",
    "    # Extract the latitude and longitude of the result\n",
    "    latitude = result[\"geometry\"][\"coordinates\"][1]\n",
    "    longitude = result[\"geometry\"][\"coordinates\"][0]\n",
    "\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
